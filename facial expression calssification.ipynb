{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 960,
     "status": "ok",
     "timestamp": 1593405297514,
     "user": {
      "displayName": "Dhaval Shah",
      "photoUrl": "",
      "userId": "07914641437881852996"
     },
     "user_tz": -330
    },
    "id": "btmbINOIA6ah",
    "outputId": "5fd0d320-3aa8-4a93-c13c-9d3f421113ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2574,
     "status": "ok",
     "timestamp": 1593405301048,
     "user": {
      "displayName": "Dhaval Shah",
      "photoUrl": "",
      "userId": "07914641437881852996"
     },
     "user_tz": -330
    },
    "id": "yA-8-llNB5dv",
    "outputId": "cb67b0e1-5c11-428b-f31e-1634804ba417"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization,Dropout\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_34KB06B0nc"
   },
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxNJJg3fCcVG"
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(32,(3,3),padding='same',input_shape=(48,48,1),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yt06DCbwCiCt"
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(64,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TV3XOswGCnrB"
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EsZKyBGrCpm6"
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(256,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qTbw-ZyPCrRE"
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSFVltqtCs97"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCYVpCozCumV"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VqN4a-OOCxZf"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=32,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdOoKQvkC1Hb"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=6,activation ='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4701,
     "status": "ok",
     "timestamp": 1593405309859,
     "user": {
      "displayName": "Dhaval Shah",
      "photoUrl": "",
      "userId": "07914641437881852996"
     },
     "user_tz": -330
    },
    "id": "YTIaPAWLC3RH",
    "outputId": "7effe893-f959-43ab-d35f-438c08effd44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 1,480,070\n",
      "Trainable params: 1,478,662\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9puFvukIC57P"
   },
   "outputs": [],
   "source": [
    "checkpoint=ModelCheckpoint('/content/gdrive/My Drive/Facial Emotion Recognition/model.h5',\n",
    "                           monitor='val_accuracy',\n",
    "                           mode='max',\n",
    "                           save_best_only=True,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzoJevUUDWEO"
   },
   "outputs": [],
   "source": [
    "earlystopping=EarlyStopping(monitor='val_accuracy',\n",
    "                            min_delta=0,\n",
    "                            patience=10,\n",
    "                            verbose=1,\n",
    "                            mode='max',\n",
    "                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1gZNQAODr8N"
   },
   "outputs": [],
   "source": [
    "callbacks=[checkpoint,earlystopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0CPSkOzDzJ4"
   },
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "                                 shear_range=0.2,\n",
    "                                 zoom_range=0.2,\n",
    "                                 horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kDEZO1vD3_q"
   },
   "outputs": [],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1593405832252,
     "user": {
      "displayName": "Dhaval Shah",
      "photoUrl": "",
      "userId": "07914641437881852996"
     },
     "user_tz": -330
    },
    "id": "MQp21McWD5yQ",
    "outputId": "7d0c3b3c-c5f5-4d42-8327-c65229296feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28393 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set=train_datagen.flow_from_directory('/content/gdrive/My Drive/Facial Emotion Recognition/Dataset/training_set',\n",
    "                                               target_size=(48,48),\n",
    "                                               color_mode='grayscale',\n",
    "                                               batch_size=32,\n",
    "                                               class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1076,
     "status": "ok",
     "timestamp": 1593405832609,
     "user": {
      "displayName": "Dhaval Shah",
      "photoUrl": "",
      "userId": "07914641437881852996"
     },
     "user_tz": -330
    },
    "id": "4pHI3uw8ERjG",
    "outputId": "9effb7ad-ff04-4da2-c03c-34b3ae7d23ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set=test_datagen.flow_from_directory('/content/gdrive/My Drive/Facial Emotion Recognition/Dataset/test_set',\n",
    "                                          target_size=(48,48),\n",
    "                                          color_mode='grayscale',\n",
    "                                          batch_size=32,\n",
    "                                          class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAci_oNiEtIy"
   },
   "outputs": [],
   "source": [
    "nb_train_images=28393\n",
    "nb_test_images=3534\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqnKgsxnEy2I"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11241428,
     "status": "ok",
     "timestamp": 1593417078218,
     "user": {
      "displayName": "Dhaval Shah",
      "photoUrl": "",
      "userId": "07914641437881852996"
     },
     "user_tz": -330
    },
    "id": "sP9DUfgDE0lm",
    "outputId": "a29a427b-0db2-42c3-feea-8f3c12980e4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "678/887 [=====================>........] - ETA: 22:27 - loss: 1.8682 - accuracy: 0.2335"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:616: UserWarning: The input 698 could not be retrieved. It could be because a worker has died.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887/887 [==============================] - 6262s 7s/step - loss: 1.8342 - accuracy: 0.2441 - val_loss: 1.6337 - val_accuracy: 0.3017\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.30170, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 2/100\n",
      "887/887 [==============================] - 89s 100ms/step - loss: 1.6072 - accuracy: 0.3458 - val_loss: 1.5019 - val_accuracy: 0.4058\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.30170 to 0.40577, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 3/100\n",
      "887/887 [==============================] - 91s 103ms/step - loss: 1.4329 - accuracy: 0.4258 - val_loss: 1.4321 - val_accuracy: 0.4569\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.40577 to 0.45688, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 4/100\n",
      "887/887 [==============================] - 89s 100ms/step - loss: 1.3177 - accuracy: 0.4833 - val_loss: 1.2188 - val_accuracy: 0.5300\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.45688 to 0.52998, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 5/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 1.2560 - accuracy: 0.5117 - val_loss: 1.0692 - val_accuracy: 0.5431\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.52998 to 0.54312, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 6/100\n",
      "887/887 [==============================] - 89s 100ms/step - loss: 1.2109 - accuracy: 0.5369 - val_loss: 1.1098 - val_accuracy: 0.5571\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.54312 to 0.55711, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 7/100\n",
      "887/887 [==============================] - 90s 101ms/step - loss: 1.1659 - accuracy: 0.5556 - val_loss: 0.9974 - val_accuracy: 0.5745\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.55711 to 0.57453, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 8/100\n",
      "887/887 [==============================] - 88s 100ms/step - loss: 1.1342 - accuracy: 0.5692 - val_loss: 1.1103 - val_accuracy: 0.5891\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.57453 to 0.58909, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 9/100\n",
      "887/887 [==============================] - 89s 100ms/step - loss: 1.1069 - accuracy: 0.5814 - val_loss: 0.8101 - val_accuracy: 0.6077\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.58909 to 0.60765, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 10/100\n",
      "887/887 [==============================] - 90s 101ms/step - loss: 1.0897 - accuracy: 0.5871 - val_loss: 1.4511 - val_accuracy: 0.5882\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.60765\n",
      "Epoch 11/100\n",
      "887/887 [==============================] - 88s 100ms/step - loss: 1.0739 - accuracy: 0.5937 - val_loss: 1.2209 - val_accuracy: 0.6162\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.60765 to 0.61622, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 12/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 1.0516 - accuracy: 0.6054 - val_loss: 1.1298 - val_accuracy: 0.5762\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.61622\n",
      "Epoch 13/100\n",
      "887/887 [==============================] - 89s 101ms/step - loss: 1.0342 - accuracy: 0.6104 - val_loss: 0.9723 - val_accuracy: 0.6374\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.61622 to 0.63735, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 14/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 1.0203 - accuracy: 0.6157 - val_loss: 1.1133 - val_accuracy: 0.6151\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.63735\n",
      "Epoch 15/100\n",
      "887/887 [==============================] - 87s 99ms/step - loss: 1.0057 - accuracy: 0.6229 - val_loss: 0.8146 - val_accuracy: 0.6288\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.63735\n",
      "Epoch 16/100\n",
      "887/887 [==============================] - 87s 98ms/step - loss: 0.9922 - accuracy: 0.6266 - val_loss: 1.0775 - val_accuracy: 0.6251\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.63735\n",
      "Epoch 17/100\n",
      "887/887 [==============================] - 90s 101ms/step - loss: 0.9824 - accuracy: 0.6363 - val_loss: 1.0118 - val_accuracy: 0.6279\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.63735\n",
      "Epoch 18/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.9694 - accuracy: 0.6396 - val_loss: 1.0023 - val_accuracy: 0.6571\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.63735 to 0.65705, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 19/100\n",
      "887/887 [==============================] - 88s 100ms/step - loss: 0.9577 - accuracy: 0.6439 - val_loss: 0.8275 - val_accuracy: 0.6385\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.65705\n",
      "Epoch 20/100\n",
      "887/887 [==============================] - 89s 100ms/step - loss: 0.9482 - accuracy: 0.6476 - val_loss: 1.0139 - val_accuracy: 0.6456\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.65705\n",
      "Epoch 21/100\n",
      "887/887 [==============================] - 89s 100ms/step - loss: 0.9411 - accuracy: 0.6513 - val_loss: 1.1605 - val_accuracy: 0.6439\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.65705\n",
      "Epoch 22/100\n",
      "887/887 [==============================] - 88s 100ms/step - loss: 0.9299 - accuracy: 0.6569 - val_loss: 0.9891 - val_accuracy: 0.6405\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.65705\n",
      "Epoch 23/100\n",
      "887/887 [==============================] - 87s 98ms/step - loss: 0.9152 - accuracy: 0.6589 - val_loss: 0.9284 - val_accuracy: 0.6496\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.65705\n",
      "Epoch 24/100\n",
      "887/887 [==============================] - 90s 102ms/step - loss: 0.9113 - accuracy: 0.6632 - val_loss: 0.6389 - val_accuracy: 0.6493\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.65705\n",
      "Epoch 25/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.9016 - accuracy: 0.6658 - val_loss: 0.6647 - val_accuracy: 0.6599\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.65705 to 0.65991, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 26/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.8942 - accuracy: 0.6718 - val_loss: 0.7785 - val_accuracy: 0.6462\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.65991\n",
      "Epoch 27/100\n",
      "887/887 [==============================] - 87s 99ms/step - loss: 0.8829 - accuracy: 0.6736 - val_loss: 0.9904 - val_accuracy: 0.6502\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.65991\n",
      "Epoch 28/100\n",
      "887/887 [==============================] - 89s 101ms/step - loss: 0.8797 - accuracy: 0.6764 - val_loss: 1.4154 - val_accuracy: 0.6653\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.65991 to 0.66533, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 29/100\n",
      "887/887 [==============================] - 87s 99ms/step - loss: 0.8739 - accuracy: 0.6796 - val_loss: 0.7405 - val_accuracy: 0.6608\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.66533\n",
      "Epoch 30/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.8621 - accuracy: 0.6827 - val_loss: 1.1725 - val_accuracy: 0.6730\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.66533 to 0.67304, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 31/100\n",
      "887/887 [==============================] - 89s 101ms/step - loss: 0.8588 - accuracy: 0.6855 - val_loss: 0.8149 - val_accuracy: 0.6522\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.67304\n",
      "Epoch 32/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.8468 - accuracy: 0.6916 - val_loss: 0.9228 - val_accuracy: 0.6562\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.67304\n",
      "Epoch 33/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.8390 - accuracy: 0.6927 - val_loss: 1.0442 - val_accuracy: 0.6585\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.67304\n",
      "Epoch 34/100\n",
      "887/887 [==============================] - 89s 101ms/step - loss: 0.8334 - accuracy: 0.6949 - val_loss: 0.9081 - val_accuracy: 0.6699\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.67304\n",
      "Epoch 35/100\n",
      "887/887 [==============================] - 89s 100ms/step - loss: 0.8257 - accuracy: 0.6984 - val_loss: 0.9014 - val_accuracy: 0.6699\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.67304\n",
      "Epoch 36/100\n",
      "887/887 [==============================] - 86s 97ms/step - loss: 0.8240 - accuracy: 0.6985 - val_loss: 0.9155 - val_accuracy: 0.6551\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.67304\n",
      "Epoch 37/100\n",
      "887/887 [==============================] - 87s 98ms/step - loss: 0.8098 - accuracy: 0.7045 - val_loss: 1.0116 - val_accuracy: 0.6596\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.67304\n",
      "Epoch 38/100\n",
      "887/887 [==============================] - 89s 101ms/step - loss: 0.8134 - accuracy: 0.7018 - val_loss: 0.8520 - val_accuracy: 0.6593\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.67304\n",
      "Epoch 39/100\n",
      "887/887 [==============================] - 88s 100ms/step - loss: 0.8003 - accuracy: 0.7076 - val_loss: 0.8007 - val_accuracy: 0.6579\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.67304\n",
      "Epoch 40/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.7937 - accuracy: 0.7097 - val_loss: 1.1206 - val_accuracy: 0.6748\n",
      "\n",
      "Epoch 00040: val_accuracy improved from 0.67304 to 0.67476, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 41/100\n",
      "887/887 [==============================] - 89s 100ms/step - loss: 0.7846 - accuracy: 0.7174 - val_loss: 1.0621 - val_accuracy: 0.6705\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.67476\n",
      "Epoch 42/100\n",
      "887/887 [==============================] - 89s 101ms/step - loss: 0.7800 - accuracy: 0.7140 - val_loss: 0.8867 - val_accuracy: 0.6776\n",
      "\n",
      "Epoch 00042: val_accuracy improved from 0.67476 to 0.67761, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 43/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.7814 - accuracy: 0.7144 - val_loss: 0.6015 - val_accuracy: 0.6693\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.67761\n",
      "Epoch 44/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.7722 - accuracy: 0.7201 - val_loss: 0.6574 - val_accuracy: 0.6685\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.67761\n",
      "Epoch 45/100\n",
      "887/887 [==============================] - 90s 102ms/step - loss: 0.7639 - accuracy: 0.7228 - val_loss: 1.2805 - val_accuracy: 0.6665\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.67761\n",
      "Epoch 46/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.7549 - accuracy: 0.7285 - val_loss: 0.7024 - val_accuracy: 0.6779\n",
      "\n",
      "Epoch 00046: val_accuracy improved from 0.67761 to 0.67790, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 47/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.7499 - accuracy: 0.7258 - val_loss: 0.6336 - val_accuracy: 0.6850\n",
      "\n",
      "Epoch 00047: val_accuracy improved from 0.67790 to 0.68504, saving model to /content/gdrive/My Drive/Facial Emotion Recognition/model.h5\n",
      "Epoch 48/100\n",
      "887/887 [==============================] - 90s 101ms/step - loss: 0.7564 - accuracy: 0.7269 - val_loss: 0.7691 - val_accuracy: 0.6699\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.68504\n",
      "Epoch 49/100\n",
      "887/887 [==============================] - 89s 101ms/step - loss: 0.7376 - accuracy: 0.7314 - val_loss: 0.6649 - val_accuracy: 0.6670\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.68504\n",
      "Epoch 50/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.7350 - accuracy: 0.7320 - val_loss: 1.0920 - val_accuracy: 0.6773\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.68504\n",
      "Epoch 51/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.7224 - accuracy: 0.7387 - val_loss: 0.6934 - val_accuracy: 0.6776\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.68504\n",
      "Epoch 52/100\n",
      "887/887 [==============================] - 90s 102ms/step - loss: 0.7251 - accuracy: 0.7399 - val_loss: 0.8674 - val_accuracy: 0.6622\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.68504\n",
      "Epoch 53/100\n",
      "887/887 [==============================] - 87s 98ms/step - loss: 0.7156 - accuracy: 0.7406 - val_loss: 1.0710 - val_accuracy: 0.6630\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.68504\n",
      "Epoch 54/100\n",
      "887/887 [==============================] - 88s 99ms/step - loss: 0.7123 - accuracy: 0.7398 - val_loss: 0.8580 - val_accuracy: 0.6830\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.68504\n",
      "Epoch 55/100\n",
      "887/887 [==============================] - 89s 101ms/step - loss: 0.7155 - accuracy: 0.7437 - val_loss: 0.4032 - val_accuracy: 0.6753\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.68504\n",
      "Epoch 56/100\n",
      "887/887 [==============================] - 89s 101ms/step - loss: 0.6979 - accuracy: 0.7469 - val_loss: 0.8875 - val_accuracy: 0.6670\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.68504\n",
      "Epoch 57/100\n",
      "887/887 [==============================] - 88s 100ms/step - loss: 0.6982 - accuracy: 0.7472 - val_loss: 0.7100 - val_accuracy: 0.6645\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.68504\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00057: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f2e500e5a58>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(training_set,\n",
    "                    steps_per_epoch=nb_train_images//batch_size,\n",
    "                    epochs=100,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=nb_test_images//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1593417148591,
     "user": {
      "displayName": "Dhaval Shah",
      "photoUrl": "",
      "userId": "07914641437881852996"
     },
     "user_tz": -330
    },
    "id": "YuZhpHR3E3tb",
    "outputId": "e231e010-ea65-4aa9-ccef-b6b6936763e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Angry': 0, 'Fear': 1, 'Happy': 2, 'Neutral': 3, 'Sad': 4, 'Surprise': 5}"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ihiJu3vEiZF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOxe8chG085FOLWtZ+HSXep",
   "collapsed_sections": [],
   "name": "facial expression calssification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
